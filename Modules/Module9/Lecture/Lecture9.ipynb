{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7daceec9-4a3c-43dd-bd28-ca02f37cf6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "library(ggplot2)\n",
    "library(tidyverse)\n",
    "library(knitr)\n",
    "library(DT)\n",
    "library(Matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c02d98-51a1-4f3f-aaad-5663fb48159f",
   "metadata": {},
   "source": [
    "# Analysis of High Dimensional Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc0cd2-ed27-4e2a-b2d5-e11f2e90123c",
   "metadata": {},
   "source": [
    "## What is high dimensional data?\n",
    "### * Capturing many data points on the same experimental unit\n",
    "### * Challenge we face in \"Omics\" data\n",
    "### - **Phenomics** - Many phenotypes measured\n",
    "### - **Genomics** - Gene expression, DNA Sequencing\n",
    "### - **Metabolomics** - HPCL-MS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2593e31a-9a3b-40b2-be50-ea1dcdb8d9b0",
   "metadata": {},
   "source": [
    "# Problems arising in high dimensional data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "612faafe-a35a-4556-9f2e-1014f4ea7666",
   "metadata": {},
   "source": [
    "## **Hypothesis testing**\n",
    "### - Conducting significance tests for multiple variables\n",
    "## **Visualization**\n",
    "### - Clustering of samples in N-dimensional space\n",
    "## **Modeling**\n",
    "### - Few Observations relative to parameters (N<<P) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795d178-8921-48df-8d97-b55be20537ca",
   "metadata": {},
   "source": [
    "# Multiple testing problem\n",
    "## Common Examples\n",
    "### - Genome-wide association (GWAS)\n",
    "### - Gene expression analysis\n",
    "\n",
    "## * If you have 10,000 tests for which the null hypothesis is true, how many times would you reject with a cut-off of $\\alpha$ < 0.05?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e067fa-bc78-448f-a9e9-f606391a8e6c",
   "metadata": {},
   "source": [
    "# Multiple Test Correction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79cb1715-5813-4ffe-80a4-57e9a9849c0a",
   "metadata": {},
   "source": [
    "### Let's assume we a drawing samples from the same population (null hypothesis is true). How often will we find a significant difference between samples?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a64b71-c0af-4f39-9c9e-1ee36ed40de8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pValsNull <- c() \n",
    "for(i in 1:10000){\n",
    "  # Draw two samples from the sample distribution (standard normal)\n",
    "  sample1 <- rnorm(10)\n",
    "  sample2 <- rnorm(10)\n",
    "  # Use a t-test to test if the two samples have statistically different means\n",
    "  pVal <- t.test(sample1, sample2)$p.value\n",
    "  # Save result in a vector PvalsNull\n",
    "  pValsNull <- c(pValsNull, pVal)\n",
    "}\n",
    "# Plot distribution of p values\n",
    "hist(pValsNull)\n",
    "# Count how many p values are less than alpha (0.05)\n",
    "sum(pValsNull < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bb9833-44c0-4676-90ac-fb22c1dded0b",
   "metadata": {},
   "source": [
    "## How often will we find a significant difference if the samples are draw from two different populations (alternative hypothesis)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5021f95a-3294-4f0c-8115-0aad6281a47d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pValsAlt <- c() \n",
    "for(i in 1:10000){\n",
    "  # Draw sample one from a standard normal\n",
    "  sample1 <- rnorm(10)\n",
    "  \n",
    "  # Draw sample two from a normal with a different mean\n",
    "  sample2 <- rnorm(10, mean=2)\n",
    "  # Use a t-test to test if the two samples have statistically different means\n",
    "  pVal <- t.test(sample1, sample2)$p.value\n",
    "  # Save result in a vector PvalsAlt\n",
    "  pValsAlt <- c(pValsAlt, pVal)\n",
    "}\n",
    "# Plot distribution of p values\n",
    "hist(pValsAlt)\n",
    "# Count how many p values are less than alpha (0.05)\n",
    "sum(pValsAlt < 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e5997-3d92-49da-ac27-df6504fab335",
   "metadata": {},
   "source": [
    "## In reality you have a mixture of null and alternative tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bf1093-0387-4b9a-8598-5ca056c2919f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pValsReality <- c()\n",
    "# On thousand tests null\n",
    "for(i in 1:1000){\n",
    "  pValsReality <- c(pValsReality,t.test(rnorm(10), rnorm(10))$p.value)\n",
    "}\n",
    "# One hundred tests alt\n",
    "for(i in 1:100){\n",
    "  pValsReality <- c(pValsReality,t.test(rnorm(10, mean=2.5), rnorm(10))$p.value)\n",
    "}\n",
    "hist(pValsReality)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7b0f3-79e7-48e9-b2f3-3b0813ff3cd5",
   "metadata": {},
   "source": [
    "# Controlling false positives\n",
    "- ### Multiple test corrections need strike a balance between Type 1 (false positive rate) and Type 2 error (lack of power)\n",
    "\n",
    "- ### Which type of error is worse?\n",
    "\n",
    "|  |Null Hypothesis|  |\n",
    "|------|----|-----|\n",
    "|      |True|False|\n",
    "|Reject|Type 1 Error| Correct|\n",
    "|Accept|Correct|Type 2  Error|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66226fb-75be-4e8e-93b7-8c185167c85d",
   "metadata": {},
   "source": [
    "## Bonferroni correction\n",
    "### - $\\frac{\\alpha}{m}$\n",
    "### - Often too conservative\n",
    "\n",
    "## Benjamini-Hochberg correction\n",
    "### - Rank p-values\n",
    "### - $\\frac{rank}{m}q$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e956f867-164e-418a-9f96-56cffb14404d",
   "metadata": {},
   "source": [
    "## Bonferroni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e1146f-58ee-4206-a1ac-e7fb8910a042",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply by m to get adjusted P values\n",
    "pValsRealityAdj <- pValsReality * 1100\n",
    "sum(pValsRealityAdj < 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b29288b-1fc6-4286-a866-74749b0b2996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using built-in R function p.adjust\n",
    "sum(p.adjust(pValsReality, method=\"bonferroni\") <  0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958d0b40-f7e9-48c3-a05d-1335eacf8fc6",
   "metadata": {},
   "source": [
    "## Benjamini-Hochberg correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307eea4d-d207-4175-9b0c-bb0cb635db60",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(p.adjust(pValsReality, method=\"BH\") <  0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e0f6bf-7391-4e5c-a081-8428260acaf9",
   "metadata": {},
   "source": [
    "# Visualizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3127af5-5ffd-451f-80e8-d47ac1c1a770",
   "metadata": {},
   "outputs": [],
   "source": [
    "kable(cars[1:10,])\n",
    "plot(cars[1:10,], xlab=\"Speed (mph)\", ylab=\"Stopping distance (ft)\", main=\"Speed vs. Stopping Distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4d8553-5021-4270-9ce2-4090ab7d6185",
   "metadata": {},
   "source": [
    "# Visualizing high dimensional data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09225608-c52d-43e4-9034-5b52e6a86b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeGene <- data.frame(Gene1=rpois(10,3),\n",
    "                       Gene2=rpois(10,4),\n",
    "                       Gene3=rpois(10,4.5),\n",
    "                       Gene4=rpois(10,6),\n",
    "                       Gene5=rpois(10,6),\n",
    "                       Gene6=rpois(10,10))\n",
    "kable(fakeGene)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94b1fb6e-3972-4e2e-bfb9-fc4e663a595d",
   "metadata": {},
   "source": [
    "# Principal components analysis\n",
    "\n",
    "- ### Finds combination of variables that produces a new axis (PC)\n",
    "- ### Designed to maximize variance explained by each PC\n",
    "- ### PCs are orthagonal to each other\n",
    "- ### Useful for visualization and can be used in a regression contexts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2d0aa6-1bde-4dc2-ab4d-7cd3f7a6a877",
   "metadata": {},
   "source": [
    "![](images/pca.webp)\n",
    "\n",
    "\n",
    "https://doi.org/10.1038/srep25696 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8afffb00-cc31-4279-86cf-0309ad6ea9c2",
   "metadata": {},
   "source": [
    "# Principal components analysis\n",
    "\n",
    "- ### $\\mathbf{X}$ is a n x m incidence matrix\n",
    "- ### $\\mathbf{X}'\\mathbf{X}$ is m x m\n",
    "- ### $S.D(\\mathbf{X}'\\mathbf{X}) = \\mathbf{U}\\mathbf{D}\\mathbf{U}'$ \n",
    "- ### $\\mathbf{U}$ contains **eigenvectors**\n",
    "- ### $\\mathbf{D}$ is a diagonal matrix of **eigenvalues**\n",
    "- ### We want the **scores** in matrix $\\mathbf{T}$\n",
    " + ### $\\mathbf{T} = \\mathbf{X}\\mathbf{U}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a1c751-cad2-41f9-9d31-59f91cc72338",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ab414b-bf83-457c-b7fd-f119630df6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "diamSub <-diamonds[1:40, c(\"carat\", \"depth\", \"table\",\n",
    "                                  \"price\", \"x\",\"y\",\"z\")]\n",
    "diamSubLabs <- diamonds[1:40,\"price\"]\n",
    "diamSubScaled <- scale(diamSub)\n",
    "pcaRes <- princomp(diamSubScaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac33421-a6af-4531-9b64-47dd6058ed43",
   "metadata": {},
   "source": [
    "## Make a Scree plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93aaf81f-4d48-48b5-9d0d-56a51c7795e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(pcaRes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d5a470-bb0b-4130-b5ec-85d8748ed657",
   "metadata": {},
   "source": [
    "## Plot the scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "599ccb64-754a-460c-b5e2-0147f3413f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "pcs <- data.frame(pc1=pcaRes$scores[,1], \n",
    "                  pc2=pcaRes$scores[,2],\n",
    "                  cut=diamSubLabs)\n",
    "pca.gg <- ggplot(pcs, aes(x=pc1, y=pc2, color=price)) +\n",
    "          geom_point()\n",
    "pca.gg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0d14d8-39a6-4891-8dad-70a28dfc3182",
   "metadata": {},
   "source": [
    "# Analyzing high dimensional data (N<<P)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b735cf9e-708e-4368-8f45-0752c515b00a",
   "metadata": {},
   "source": [
    "## Simulate data\n",
    "\n",
    "The following section of code simulates multiple continuous covariates and the corresponding phenotypes. This data set will be used to explore various methods for dealing with high-dimensional data. The simulated data set is small for demonstration purposes, but is generated to show the issues that arise from over-parameterization and how various modeling approaches deal with these challenges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53821804-2a43-4f76-b543-3aa63bd4d924",
   "metadata": {},
   "outputs": [],
   "source": [
    "set.seed(100)\n",
    "# generate multiple continuous independent variables for 100 observations\n",
    "#initializing the full incidence matrix\n",
    "Xfull=matrix(0,100,70)\n",
    "for(i in c(1:70)){\n",
    "  Xfull[,i]=rnorm(100,0,1)\n",
    "}\n",
    "# Xfull now has 70 independent covariates that were generated with no correlation structure\n",
    "# generate phenotypes that are a function of the randomly generated covariates\n",
    "# sample beta values\n",
    "betaTrue=runif(70,.1,1)\n",
    "#generate phenotpyes\n",
    "y=Xfull%*%betaTrue+rnorm(100,0,(.5*var(Xfull%*%betaTrue))**.5)\n",
    "var(y)\n",
    "var(Xfull%*%betaTrue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3532fca-34d4-46b7-ad56-39b7d3180987",
   "metadata": {},
   "source": [
    "## Creating an over-parameterized situation\n",
    "\n",
    "The dataset as simulated is not overparameterized - the number of parameters is less than the number of independent observation used to solving for the unknown effects. we will generate an over-parameterized dataset by taking a random subset of data such that the number of independent of observations is less than the number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba0c4ca9-0020-49df-b396-c4551ed2d1e3",
   "metadata": {},
   "source": [
    "## First let's get the OLS estimates for beta using the full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2972e6fe-c6f0-4ebd-a3d1-4eb55ce6642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "XfXf=t(Xfull)%*%(Xfull)\n",
    "#XfXf matrix is full rank so we can invert and solve for bhat\n",
    "rankMatrix(XfXf)\n",
    "# solving for bhat\n",
    "bhat=solve(XfXf)%*%t(Xfull)%*%y\n",
    "hist(bhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "349eced0-7495-4db4-92c2-fe0cd9441bd7",
   "metadata": {},
   "source": [
    "Now let's take a subset of this data to create an over parameterized dataset-\n",
    "where p > n. To do this we will create y.train which we will use to estimate bhat and y.validation that we will use to test how good the estimates are at predicting the value of observations that were not used to get the solution.\n",
    "Cross-validation is a good way to examine overfitting, a common problem with over parameterized data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2adec7-42bb-4b42-a920-88a200b342e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training set from initial data set\n",
    "Xt=Xfull[1:30,]\n",
    "y.train=y[1:30]\n",
    "\n",
    "# Create validation set from initial data set\n",
    "Xv=Xfull[31:100,]\n",
    "y.validation=y[31:100]\n",
    "\n",
    "#now let's try to solve for bhat using the reduced data set\n",
    "XtXt=t(Xt)%*%(Xt)\n",
    "rankMatrix(XtXt)\n",
    "#bhat=solve(XtXt)%*%t(Xt)%*%y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e91194-2d42-41cc-8548-e3f757679a46",
   "metadata": {},
   "source": [
    "# Ridge Regression\n",
    "XtXt is a 70x70 matrix with a rank of 30 (we only have 30 observations). As a result XtXt is singular meaning we cannot invert it and we cannot get solutions for bhat. We need to find a way to solve for bhat as best we can while dealing with the issue of over-parameterization. Feature selection is one option, but in this case we know each covariate explains variation in y, so eliminating 40 of these covariates to get a full rank XtXt is not an ideal solution.\n",
    "\n",
    "Ridge regression works by adding some constant value to the diagonal of XtXt to enable us to invert it and solve. Adding a value to the diagonal means our solutions are no longer unbiased - so not good if we are using the estimates for hypothesis testing, but bias in estimates are acceptable if the goal is prediction and introducing the bias leads to good predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e7807b2-8f18-45ae-a101-1217272d1c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly choosing the constant to add to the diagonal. In practice you would want to find the constant that maximizes prediction accuracy (usually measure using cross-validation)\n",
    "RRconstant=10\n",
    "# Adding constant\n",
    "RRXtXt=XtXt + diag(RRconstant,70,70)\n",
    "# testing to see if it is full rank\n",
    "rankMatrix(RRXtXt)\n",
    "\n",
    "# Given RRXtXt is full rank let's go ahead and solve for bhat\n",
    "bhatRR=solve(RRXtXt)%*%t(Xt)%*%y.train\n",
    "\n",
    "# generating predicted yhat values for the 70 obseervation we left out of the training dataset\n",
    "yhatv=Xv%*%bhatRR\n",
    "\n",
    "# calculating the correlation\n",
    "cor(yhatv,y.validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "672910c0-b3e1-4a37-ba63-0698286dca9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(bhatRR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1151de5d-d7f7-4f6e-914a-c5ef1d8f332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cor(bhatRR,bhat)\n",
    "plot(bhatRR,bhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860dce4a-3851-4677-a076-f9db0aeafc0c",
   "metadata": {},
   "source": [
    "## Principle Component Regression\n",
    "\n",
    "Unlike penalized methods that fit all covariates but use a penalty to prevent overfitting, PCR seeks to reduce the dimensions of the data. Rather than removing the columns of X as would be done using feature selection, PCR decomposes X into orthogonal components and retains components that explain the majority of variation in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e33180-07d7-4258-8b4a-633fef012298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this example we will use a spectral decomposition of X'X to form a matrix T wich contains orthagonal vectors (columns)\n",
    "#first let's center X - given the values in X were simulated to have a mean of zero this won't have much of an impact but it is best pratice to do so.\n",
    "Xtc=scale(Xt, center=TRUE, scale=F)\n",
    "# Here we use the function eigen to decompose X'X\n",
    "XtcXtc=t(Xtc)%*%Xtc\n",
    "E=eigen(XtcXtc)\n",
    "# E is an object that contains both the eigen values and the eigen vectors\n",
    "# Lets plot the eigen values\n",
    "plot(E$values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66bb6084-6c76-4ec6-bc0f-d9bb37aeba91",
   "metadata": {},
   "source": [
    "Since Xtc'Xtc in this case has a rank of 30, the first 30 eigenvectors explain all of the variation in Xtc. Our next step is to construct a full rank matrix with orthogonal columns (T) and the solve for OLS using T."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da9c564-74f8-48b3-b093-ad22f0b4bbb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's pull the eigen vectors out of the object E\n",
    "U=E$vectors\n",
    "# We now want to set up P such that T'T can be inverted. For starters let's take the first 29 vectors in U\n",
    "P=U[,1:29]\n",
    "# Now we calculate T as T=XP\n",
    "T=Xtc%*%P\n",
    "# Solving OLS using T\n",
    "bpcr=solve(t(T)%*%T)%*%t(T)%*%y.train\n",
    "# Now let's do cross validation to see how good our predictor is\n",
    "Xvc=scale(Xv, center=TRUE, scale=F)\n",
    "Tv=Xvc%*%P\n",
    "yhatvpcr=Tv%*%bpcr\n",
    "cor(yhatvpcr,y.validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fa096a-069c-42b1-821c-593ff4c9ac3c",
   "metadata": {},
   "source": [
    "## Ridge regression is a penalized method\n",
    "- ### Random effects in a mixed model are also a penalized method and can be used to prevent overfitting\n",
    "- ### When working with high-dimensional data penalized methods are commonly used\n",
    "    - ### Methods differ based on the type of penalty and how it is estimated\n",
    "    - ### For mixed models the penalty is $\\frac{\\sigma^2_{e}}{\\sigma^2_{u}}$ \n",
    "- ### Genomic prediction is a common breeding application for penalized methods\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bca68053-2519-4e24-ad8b-33f264f0a113",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "4.3.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
